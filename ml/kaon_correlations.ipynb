{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIYgT9TRvqdo"
   },
   "source": [
    "# Cuts Optimization's correlations\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASseI7LRvGdo"
   },
   "source": [
    "# Installing Libraries\n",
    "There are certain libraries that we use but do not come in with the normal version of python, therefore, they are needed to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFubPhT4BjiW"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install hipe4ml\n",
    "!pip install uproot\n",
    "!pip install xxhash\n",
    "!pip install lz4\n",
    "!pip install plotly\n",
    "!pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8eyD-mRvduz"
   },
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. **Matplotlib** and **Seaborn** come handy in plotting and visualizing the data. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook. We will also need some functions of **Scipy** for fittintg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezPoWUMUvfap",
    "outputId": "d58741b3-e722-4e4f-c797-de603ab37229"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sn\n",
    "from hipe4ml.tree_handler import TreeHandler\n",
    "#from hipe4ml import plot_utils\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from scipy.stats import binned_statistic as b_s\n",
    "from scipy.stats import linregress\n",
    "from sklearn import datasets, linear_model\n",
    "import statsmodels as sm\n",
    "import uproot\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7FjmDOcv2OB"
   },
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **TTree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "A lambda baryon mostly decays into a proton and a pion. In this example, we download two files that were converted to a plain TTree format (simplest structure to be read by Python). The first one contains mostly background candidates for lambda i.e. protons and pions tracks which do not come from a lambda decay. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwJznuSwv3UQ"
   },
   "outputs": [],
   "source": [
    "#data will be imported locally\n",
    "#%%capture\n",
    "#!curl -L https://cernbox.cern.ch/index.php/s/AnY17jXGwSpp6QQ/download --output bg.root\n",
    "#!curl -L https://cernbox.cern.ch/index.php/s/RHOSK5CluLf9yso/download --output signal.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQ6rwB3v_0R"
   },
   "source": [
    "The following lines of code converts the root files into a pandas dataframe objects. With the help of a selection cut, we select only signal candidates and background candidates from their respective data sets. Another selection cut chooses lambda candidates only in the $\\pm 5\\sigma$ region around the mean of the lambda mass peak. Similarly, we select the background candidates outside this $\\pm 5\\sigma$ region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKnmnX_TwAig"
   },
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "signal = pd.DataFrame(data=uproot.open('PlainTree_20kSign.root:PlainTree').arrays(library='np'))\n",
    "\n",
    "# We only select lambda candidates in the 5 sigma region around the kaon mass peak\n",
    "signal = signal[(signal['Candidates_generation']==1) & (signal['Candidates_mass']>0.43485) & (signal['Candidates_mass']<0.56135)]\n",
    "\n",
    "# Similarly for the background, we select background candidates which are not in the 5 sigma region of the kaon peak\n",
    "background = pd.DataFrame(data=uproot.open('PlainTree_20kBckgr.root:PlainTree').arrays(library='np'))\n",
    "background = background[(background['Candidates_generation'] < 1)\n",
    "                & ((background['Candidates_mass'] > 0.3)\n",
    "                & (background['Candidates_mass'] < 0.43485) | (background['Candidates_mass']>0.56135) \n",
    "                   & (background['Candidates_mass'] < 0.7))]\n",
    "\n",
    "#Also call the garbage collector of python to collect unused items to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we remove name prefixes 'Candidates'\n",
    "background.columns = background.columns.str.replace('Candidates_', '')\n",
    "background.columns = background.columns.str.replace('_', '')\n",
    "signal.columns = signal.columns.str.replace('Candidates_', '')\n",
    "signal.columns = signal.columns.str.replace('_', '')\n",
    "#let's check name prefixes \n",
    "background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WHuzi6GxB1L"
   },
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which are outliers or does not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. \n",
    "\n",
    "Similarly, CBM is a fixed target experiment so there are certain conditions which the data has to satisfy before it is considered as reliable data.So we apply certain limits on the data sets.\n",
    "\n",
    "Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEmS1939xEDS"
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # let's treat all the infinite, inf, values by nan and then we drop all the null entries\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    #Experimental constraints\n",
    "    is_good_mom = (df['pz'] > 0) & (df['p']<20) & (df['pT']<3)\n",
    "    is_good_coord = (abs(df['x']) < 100) & (abs(df['y']) < 100) & (df['z']>0) & (df['z']<80)\n",
    "    is_good_params = (df['distance'] > 0) & (df['distance'] < 100) & (df['chi2geo']>0) & (df['chi2geo'] < 1000) & (df['chi2topo'] > 0) & (df['chi2topo'] < 100000) & (df['eta']>1) & (df['eta']<6.5) & (df['l']<80) & (df['loverdl']>0) & (df['loverdl']<5000)\n",
    "    is_good_daughters = (df['chi2primfirst']>0) & (df['chi2primfirst'] < 3e7) & (df['chi2primsecond']>0) & (df['chi2primsecond']<1e6)\n",
    "    is_good_mass = (df['mass']>0.279) & (df['mass']<2)\n",
    "\n",
    "    is_good_df = (is_good_mom) & (is_good_coord) & (is_good_params) & (is_good_daughters) & (is_good_mass)\n",
    "\n",
    "    return df[is_good_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpWTegAtxF9F"
   },
   "outputs": [],
   "source": [
    "background = clean_df(background)\n",
    "signal = clean_df(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2muqkDXBxJLS"
   },
   "source": [
    "# Correlation\n",
    "We find the correlation of all variables with signal and background candidates. We use the pearson correlation coefficient (linear correlation) for our analysis. It is defined as \n",
    "$$\n",
    "\\rho = \\frac{COV(X,Y)}{\\sigma_X \\times \\sigma_Y}\n",
    "$$\n",
    "Here, COV(X,Y) is the covariance of the variable X and Y, and $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of the variables. Pearson co-efficient is useful for linear correlation but it fails to take into account outliers and non-linear correlation. $\\rho \\> 0$ means postive while the opposite means negative correlation between two variables. \n",
    "\n",
    "This correlation function comes in built in the pandas library so we are using it. This function can also find other non-linear correlation coefficients like kendall and spearman. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJce4HLo2R8d"
   },
   "outputs": [],
   "source": [
    "variables_to_draw = ['chi2geo', 'chi2primfirst', 'chi2primsecond', 'chi2topo', 'cosinefirst',\n",
    "       'cosinesecond', 'cosinetopo', 'distance', 'eta', 'l', 'loverdl',\n",
    "       'mass', 'p', 'pT', 'phi', 'px', 'py', 'pz', 'rapidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYdjR7kcxLut"
   },
   "outputs": [],
   "source": [
    "def correlation_graph(df, variables):\n",
    "    # The variables pid, isfrompv and issignal are not that much varying so we remove them\n",
    "    new_df = df[variables]\n",
    "    # Using the pandas correlation function corr we find the correlation\n",
    "    df_correlation_all = new_df.corr(method='pearson')\n",
    "    \n",
    "    #The cosmetics of the graph\n",
    "    fig, ax = plt.subplots(figsize=(20,15))  #figure size\n",
    "    cmap = sn.diverging_palette(240, 10, as_cmap=True, n=200) #color map\n",
    "    cax = sn.heatmap(df_correlation_all, annot=True,cbar_kws={\"shrink\": .5},  cmap=cmap,  vmin=-1, vmax=1, \n",
    "                 center=0)\n",
    "    ax.set_xticks(np.arange(0, len(df_correlation_all.columns), step=1))\n",
    "    ax.set_xticklabels(df_correlation_all.columns, fontsize=15, rotation =70)\n",
    "    ax.set_yticklabels(df_correlation_all.columns, fontsize=15)\n",
    "    ax.set_title('Correlations of all variables', fontsize = 20)\n",
    "    fig.savefig(\"correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "id": "e3fDso5jxQfr",
    "outputId": "56c06555-baf3-4217-9366-68e091d27f52"
   },
   "outputs": [],
   "source": [
    "# Insert the data frame object in the following brackets and it will generate the correlation graph for it. Right now use either signal or background\n",
    "correlation_graph(background, variables_to_draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwInaQavinph"
   },
   "source": [
    "The correlation graph of the background variables shows that cosinepos is correlated with mass. To check whether it is a real correlation or a statistical fluctuation we make our own correlation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvNhNiETxTOI"
   },
   "source": [
    "## Correlations by formula\n",
    "The following function calculates the correlation along with the standard error of the mean (SEM) of the input variable with all the other variables. The standard error of the mean is defined as $ SEM = \\frac{\\sigma}{\\sqrt{n}}$. Here $\\sigma$ is the standard deviation of a variable. It will put error bars on each bin.\n",
    "\n",
    "The function accepts 3 variables, a data frame object in the first input, a list of strings to be correlated with the third input (a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHw71Om-xVIR"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation(df, vars_to_corr, target_var) :\n",
    "    \n",
    "    from scipy.stats import sem\n",
    "\n",
    "    mean = df[target_var].mean()\n",
    "    sigma = df[target_var].std()\n",
    "\n",
    "    correlation = []\n",
    "    error = []\n",
    "    \n",
    "    for j in vars_to_corr : \n",
    "        mean_j = df[j].mean()\n",
    "        sigma_j = df[j].std()\n",
    "        \n",
    "        cov = (df[j] - mean_j) * (df[target_var] - mean) / (sigma*sigma_j)        \n",
    "        correlation.append(cov.mean())\n",
    "        error.append(sem(cov))\n",
    "    \n",
    "    return correlation, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHsuo8LnxeLT"
   },
   "outputs": [],
   "source": [
    "# Provide the data frame object first, then also inside the brackets of list and then write the variable inside inverted commas ''.\n",
    "# For signal\n",
    "corr_signal, corr_signal_errors = calculate_correlation(signal, list(signal), 'mass')\n",
    "# For background\n",
    "corr_bg, corr_bg_errors = calculate_correlation(background, list(background), 'mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "VOVyyAItxkBV",
    "outputId": "210ea05a-71a6-49df-8f5a-f03b020989b8"
   },
   "outputs": [],
   "source": [
    "# Plotting the correlations of various variables with mass along with the errors\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.errorbar(list(signal), corr_signal, yerr=corr_signal_errors, fmt='')\n",
    "plt.errorbar(list(background), corr_bg, yerr=corr_bg_errors, fmt='')\n",
    "ax.grid(zorder=0)\n",
    "ax.set_xticklabels(signal.columns, fontsize=15, rotation =90)\n",
    "plt.legend(('signal','background'), fontsize = 15)\n",
    "fig.tight_layout()\n",
    "plt.title('Correlation of all variables with mass along with SEM', fontsize = 15)\n",
    "#fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHfQ5kiSfKJe"
   },
   "source": [
    "2d histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgPfpVkA57Vm"
   },
   "source": [
    "## Scatter plot between variables\n",
    "To analyze the correlation between the mass variable and the cosine of the angle between the proton and the lambda for the background set, we multi-differential analysis.\n",
    "\n",
    "We make a function which takes in a data frame object in the first input, and then two variables from this df in the next inputs. This function takes the entries of the variables and distributes them in 100 bins. The function then plots the bin centers of the first variable on the x-axis and the mean values of the bins of the second variable on the y-axis, along with its bin stds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oxrvleaml3S8"
   },
   "outputs": [],
   "source": [
    "#for variable bin size, use the following bins\n",
    "non_uniform_binning = [1.07032418, 1.07962069, 1.08891719, 1.0982137 , 1.1075102 ,\n",
    "       1.11680671, 1.12610322, 1.13539972, 1.14469623, 1.15399273,\n",
    "       1.16328924, 1.17258574, 1.18188225, 1.19,1.20977176, 1.25625429,\n",
    "       1.30273682, 1.34921935, 1.39570187, 1.4421844 , 1.48866693,\n",
    "       1.53514946, 1.58163198, 1.62811451, 1.67459704, 1.72107956,\n",
    "       1.76756209, 1.81404462, 1.86052715, 1.90700967, 1.9534922 ,\n",
    "       1.99997473]\n",
    "bb = ['1.07', '', '', '', '1.1', '', '', '1.13', '', '', '', '', '', '', '1.2', '1.25', '1.3', '1.34', '1.39', '1.44', '1.48', '1.53', '1.58', '1.62', '1.67', '1.72', '1.76', '1.814', '1.86', '1.9', '1.953', '1.99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7Avnp5HxrMs"
   },
   "outputs": [],
   "source": [
    "def profile_plot(df,variable_xaxis,variable_yaxis, binning):\n",
    "    fig, axs = plt.subplots(figsize=(20, 15))\n",
    "    # Distributing the data into 100 bins\n",
    "    bin_means, bin_edges, binnumber = b_s(df[variable_xaxis],df[variable_yaxis], statistic='mean', bins=binning)\n",
    "    bin_std, bin_edges, binnumber = b_s(df[variable_xaxis],df[variable_yaxis], statistic='std', bins=binning)\n",
    "    bin_count, bin_edges, binnumber = b_s(df[variable_xaxis],df[variable_yaxis], statistic='count', bins=binning)\n",
    "    bin_width = (bin_edges[1] - bin_edges[0])\n",
    "    bin_centers = bin_edges[1:] - bin_width/2\n",
    "    plt.errorbar(x=bin_centers, y=bin_means, yerr=(bin_std/np.sqrt(bin_count)), linestyle='none', marker='.',mfc='red', ms=10)\n",
    "    # Fitting a line on the data  \n",
    "    X = bin_centers\n",
    "    y = bin_means\n",
    "    X2 = sm.add_constant(X) \n",
    "    est = sm.OLS(y, X2)\n",
    "    est2 = est.fit()\n",
    "    p_value1=str(est2.pvalues[0])\n",
    "    p_value2=str(est2.pvalues[1])\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x=bin_centers, y=bin_means)\n",
    "    print(\"summary()\\n\",est2.summary())\n",
    "    print(\"pvalues\\n\",est2.pvalues)\n",
    "    print(\"tvalues\\n\",est2.tvalues)\n",
    "    print(\"rsquared\\n\",est2.rsquared)\n",
    "    print(\"rsquared_adj\\n\",est2.rsquared_adj)\n",
    "    print('sum of squared residuals = ', np.sum(est2.resid))\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print('mean squared error: ',mean_squared_error(bin_means, intercept + slope*bin_centers))\n",
    "\n",
    "    predictions = est2.predict(X2)\n",
    "\n",
    "    print(est2.predict(X2[:3,:]))\n",
    "\n",
    "    plt.vlines(x=1.115,ymin=0.96,ymax=1.01, color='r', linestyle='-')\n",
    "  \n",
    "    #plotting\n",
    "    plt.plot(bin_centers, intercept + slope*bin_centers, 'b', label='fitted line'+\n",
    "             \" with $R^2$-squared: %f\" % r_value**2+'\\n and the p values are \\n ['+p_value1+'  '+ p_value2+'] \\n $\\chi^2$')\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.title('Mean of ' +variable_yaxis+ ' plotted versus bin centers of '+variable_xaxis, fontsize=18)\n",
    "    plt.xlabel(\"Bin centers\", fontsize=18)\n",
    "    plt.ylabel(\"Mean of each bin with the SEM ($\\dfrac{bin\\ std}{\\sqrt{bin\\ count}}$) of bin\", fontsize=18)\n",
    "    #for non-uniform binning labels\n",
    "    axs.set_xticks(non_uniform_binning)\n",
    "    axs.set_xticklabels(bb)\n",
    "    axs.tick_params(labelsize=18)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5LeQ7GVzxwfE",
    "outputId": "4489af6a-6275-4ae2-f2a0-5b5b9ce4e927"
   },
   "outputs": [],
   "source": [
    "profile_plot(background,'mass','cosinepos',non_uniform_binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yoBSKGI_Jdy"
   },
   "outputs": [],
   "source": [
    "def two_D_hist(var_xaxis, var_yaxis):\n",
    "    import matplotlib as mpl\n",
    "    fig, axs = plt.subplots(figsize=(15, 10))\n",
    "    plt.hist2d(var_xaxis,var_yaxis, bins=100, norm=mpl.colors.LogNorm())\n",
    "    plt.xlabel(var_xaxis.name, fontsize=15)\n",
    "    plt.ylabel(var_yaxis.name, fontsize=15)\n",
    "    plt.title(\"2D histogram having \"+var_xaxis.name +\" on the x-axis and \"+var_yaxis.name+\" on the y-axis\", fontsize=15)\n",
    "    axs.tick_params(labelsize=18)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "nu0mjBis_Lb3",
    "outputId": "8620096e-ced6-4d0e-968d-971d191f20b3"
   },
   "outputs": [],
   "source": [
    "two_D_hist(background['mass'], background['cosinepos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqvxU3MibRBo"
   },
   "source": [
    "#3D plot\n",
    "This one is generated for just fun, try to play around with three variables at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "uTKvTkKlbK9t",
    "outputId": "47d880e4-e110-43dc-916b-9ef3a91814b8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Configure Plotly to be rendered inline in the notebook.\n",
    "\n",
    "# Configure the trace.\n",
    "trace = go.Scatter3d(\n",
    "    x=background['mass'],  # <-- Put your data instead\n",
    "    y=background['pT'],  # <-- Put your data instead\n",
    "    z=background['cosinepos'],  # <-- Put your data instead\n",
    "    mode='markers',\n",
    "    marker={\n",
    "        'size': 1,\n",
    "        'opacity': 0.8,\n",
    "    },\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Configure the layout.\n",
    "layout = go.Layout(scene = dict(xaxis_title='mass',yaxis_title='PT', zaxis_title='cosinepos',\n",
    "        xaxis = dict(nticks=6, range=[1.09,2]),\n",
    "                     yaxis = dict(nticks=6, range=[0,3],),\n",
    "                     zaxis = dict(nticks=6, range=[0.5,1],),),\n",
    "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Render the plot.\n",
    "plotly.offline.iplot(plot_figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2xezSM7dqLg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CBM collaboration meeting correlations.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
