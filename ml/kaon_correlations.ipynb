{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLAINTREE FILES NAMES\n",
    "signalFileName = 'PlainTree_20kSign.root:PlainTree'\n",
    "backgroundFileName ='PlainTree_20kBckgr.root:PlainTree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUTS FOR DATA CLEANING\n",
    "#5 sigma region for signal\n",
    "lower5SigmaCutSign = 0.43485\n",
    "upper5SigmaCutSign = 0.56135\n",
    "# \"5sigma\" (not acutal 5 sigma) region for background\n",
    "lower5SigmaCutBckgr = 0.1\n",
    "upper5SigmaCutBckgr = 2\n",
    "#mass cuts for both bckgr and sign\n",
    "lowerMassCut = 0.279\n",
    "upperMassCut = 1.5\n",
    "#distance cuts\n",
    "#DCA\n",
    "lowerDcaCut = 0\n",
    "upperDcaCut = 100\n",
    "#l distance\n",
    "lowerLCut = 0\n",
    "upperLCut = 80\n",
    "#loverdl\n",
    "lowerLdlCut = 0\n",
    "upperLdlCut = 5000\n",
    "#coordinate cuts\n",
    "absXCut = 50\n",
    "absYCut = 50\n",
    "lowerZCut = 0\n",
    "upperZCut = 50\n",
    "#momentums cuts\n",
    "pzLowerCut = 0\n",
    "pUpperCut = 20\n",
    "ptUpperCut = 3\n",
    "#chi2\n",
    "#geo\n",
    "lowerChi2GeoCut = 0\n",
    "upperChi2GeoCut = 1000\n",
    "#topo\n",
    "lowerChi2TopoCut = 0\n",
    "upperChi2TopoCut = 100000\n",
    "#prim first\n",
    "lowerChi2PrimFirstCut = 0\n",
    "upperChi2PrimFirstCut = 3e7\n",
    "#prim second\n",
    "lowerChi2PrimSecondCut = 0\n",
    "upperChi2PrimSecondCut = 3e7\n",
    "#pseudorapidity cuts\n",
    "lowerEtaCut = 1\n",
    "upperEtaCut = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8eyD-mRvduz"
   },
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. **Matplotlib** and **Seaborn** come handy in plotting and visualizing the data. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook. We will also need some functions of **Scipy** for fittintg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezPoWUMUvfap",
    "outputId": "d58741b3-e722-4e4f-c797-de603ab37229",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sn\n",
    "from hipe4ml.tree_handler import TreeHandler\n",
    "#from hipe4ml import plot_utils\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from scipy.stats import binned_statistic as b_s\n",
    "from scipy.stats import linregress\n",
    "from sklearn import datasets, linear_model\n",
    "import statsmodels as sm\n",
    "import uproot\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7FjmDOcv2OB",
    "tags": []
   },
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **TTree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "A lambda baryon mostly decays into a proton and a pion. In this example, we download two files that were converted to a plain TTree format (simplest structure to be read by Python). The first one contains mostly background candidates for lambda i.e. protons and pions tracks which do not come from a lambda decay. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQ6rwB3v_0R"
   },
   "source": [
    "The following lines of code converts the root files into a pandas dataframe objects. With the help of a selection cut, we select only signal candidates and background candidates from their respective data sets. Another selection cut chooses lambda candidates only in the $\\pm 5\\sigma$ region around the mean of the lambda mass peak. Similarly, we select the background candidates outside this $\\pm 5\\sigma$ region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKnmnX_TwAig"
   },
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "signal = pd.DataFrame(data=uproot.open(signalFileName).arrays(library='np'))\n",
    "\n",
    "# We only select lambda candidates in the 5 sigma region around the kaon mass peak\n",
    "#we preserve the cleaned dataframe with a changed name\n",
    "sign = signal[(signal['Candidates_generation']==1) & (signal['Candidates_mass']>lower5SigmaCutSign) & (signal['Candidates_mass']<upper5SigmaCutSign)]\n",
    "\n",
    "# Similarly for the background, we select background candidates which are not in the 5 sigma region of the kaon peak\n",
    "background = pd.DataFrame(data=uproot.open(backgroundFileName).arrays(library='np'))\n",
    "#we preserve the cleaned dataframe with a changed name\n",
    "bckgr = background[(background['Candidates_generation'] < 1)\n",
    "                & ((background['Candidates_mass'] > lower5SigmaCutBckgr)\n",
    "                & (background['Candidates_mass'] < lower5SigmaCutSign) | (background['Candidates_mass']>upper5SigmaCutSign) \n",
    "                   & (background['Candidates_mass'] < upper5SigmaCutBckgr))]\n",
    "\n",
    "#Also call the garbage collector of python to collect unused items to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we remove name prefixes 'Candidates'\n",
    "bckgr.columns = bckgr.columns.str.replace('Candidates_', '')\n",
    "bckgr.columns = bckgr.columns.str.replace('_', '')\n",
    "sign.columns = sign.columns.str.replace('Candidates_', '')\n",
    "sign.columns = sign.columns.str.replace('_', '')\n",
    "#we also get rid of coordinates errors\n",
    "sign = sign.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'generation', 'pid'])\n",
    "bckgr = bckgr.drop(columns=['xerror', 'yerror', 'zerror', 'daughter1id', 'daughter2id', 'generation', 'pid'])\n",
    "#let's check the name prefixes \n",
    "sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WHuzi6GxB1L"
   },
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which are outliers or does not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. \n",
    "\n",
    "Similarly, CBM is a fixed target experiment so there are certain conditions which the data has to satisfy before it is considered as reliable data.So we apply certain limits on the data sets.\n",
    "\n",
    "Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEmS1939xEDS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # let's treat all the infinite, inf, values by nan and then we drop all the null entries\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    #Experimental constraints\n",
    "    is_good_mom = (df['pz'] > pzLowerCut) & (df['p']<pUpperCut) & (df['pT']<ptUpperCut)\n",
    "    is_good_coord = (abs(df['x']) < absXCut) & (abs(df['y']) < absYCut) & (df['z']>lowerZCut) & (df['z']<upperZCut)\n",
    "    is_good_params = (df['distance'] > lowerDcaCut) & (df['distance'] < upperDcaCut) & (df['chi2geo']>lowerChi2GeoCut) & (df['chi2geo'] < upperChi2GeoCut) & (df['chi2topo'] > lowerChi2TopoCut) & (df['chi2topo'] < upperChi2TopoCut) & (df['eta']>lowerEtaCut) & (df['eta']<upperEtaCut)& (df['l']>lowerLCut) & (df['l']<upperLCut) & (df['loverdl']>lowerLdlCut) & (df['loverdl']<upperLdlCut)\n",
    "    is_good_daughters = (df['chi2primfirst']>lowerChi2PrimFirstCut) & (df['chi2primfirst'] < upperChi2PrimSecondCut) & (df['chi2primsecond']>lowerChi2PrimSecondCut) & (df['chi2primsecond']<upperChi2PrimFirstCut)\n",
    "    is_good_mass = (df['mass']>lowerMassCut) & (df['mass']<upperMassCut)\n",
    "\n",
    "    is_good_df = (is_good_mom) & (is_good_coord) & (is_good_params) & (is_good_daughters) & (is_good_mass)\n",
    "\n",
    "    return df[is_good_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpWTegAtxF9F"
   },
   "outputs": [],
   "source": [
    "#we'll count how much data we loose\n",
    "bckgrCount = len(bckgr)\n",
    "signCount = len(sign)\n",
    "#we return to normal names while cleaning data\n",
    "background = clean_df(bckgr)\n",
    "signal = clean_df(sign)\n",
    "backgroundCount = len(background)\n",
    "signalCount = len(signal)\n",
    "#lets count how much data we lose\n",
    "backgroundDifference = bckgrCount-backgroundCount\n",
    "signalDifference = signCount-signalCount\n",
    "percentageBg = backgroundDifference/bckgrCount*100\n",
    "percentageSg = signalDifference/signCount*100\n",
    "#finally\n",
    "print('we lost ' + str(backgroundDifference)+' background entries (' + str(round(percentageBg, 2)) + '%) and ' + str(signalDifference) + ' signal entries (' + str(round(percentageSg)) + '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2muqkDXBxJLS"
   },
   "source": [
    "# Correlation\n",
    "We find the correlation of all variables with signal and background candidates. We use the pearson correlation coefficient (linear correlation) for our analysis. It is defined as \n",
    "$$\n",
    "\\rho = \\frac{COV(X,Y)}{\\sigma_X \\times \\sigma_Y}\n",
    "$$\n",
    "Here, COV(X,Y) is the covariance of the variable X and Y, and $\\sigma_X$ and $\\sigma_Y$ are the standard deviations of the variables. Pearson co-efficient is useful for linear correlation but it fails to take into account outliers and non-linear correlation. $\\rho \\> 0$ means postive while the opposite means negative correlation between two variables. \n",
    "\n",
    "This correlation function comes in built in the pandas library so we are using it. This function can also find other non-linear correlation coefficients like kendall and spearman. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJce4HLo2R8d"
   },
   "outputs": [],
   "source": [
    "variables_to_draw = ['chi2geo', 'chi2primfirst', 'chi2primsecond', 'chi2topo', 'cosinefirst',\n",
    "       'cosinesecond', 'cosinetopo', 'distance', 'eta', 'l', 'loverdl',\n",
    "       'mass', 'p', 'pT', 'phi', 'px', 'py', 'pz', 'rapidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYdjR7kcxLut"
   },
   "outputs": [],
   "source": [
    "def correlation_graph(df, variables, title):\n",
    "    # The variables pid, isfrompv and issignal are not that much varying so we remove them\n",
    "    new_df = df[variables]\n",
    "    # Using the pandas correlation function corr we find the correlation\n",
    "    df_correlation_all = new_df.corr(method='pearson')\n",
    "    \n",
    "    #The cosmetics of the graph\n",
    "    fig, ax = plt.subplots(figsize=(20,15))  #figure size\n",
    "    cmap = sn.diverging_palette(240, 10, as_cmap=True, n=200) #color map\n",
    "    cax = sn.heatmap(df_correlation_all, annot=True,cbar_kws={\"shrink\": .5},  cmap=cmap,  vmin=-1, vmax=1, \n",
    "                 center=0)\n",
    "    ax.set_xticks(np.arange(0, len(df_correlation_all.columns), step=1))\n",
    "    ax.set_xticklabels(df_correlation_all.columns, fontsize=15, rotation =70)\n",
    "    ax.set_yticklabels(df_correlation_all.columns, fontsize=15)\n",
    "    ax.set_title(title, fontsize = 20)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "id": "e3fDso5jxQfr",
    "outputId": "56c06555-baf3-4217-9366-68e091d27f52"
   },
   "outputs": [],
   "source": [
    "#correlation graph for background\n",
    "correlation_graph(background, variables_to_draw, 'Correlations of all variables for background (pearson)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "id": "e3fDso5jxQfr",
    "outputId": "56c06555-baf3-4217-9366-68e091d27f52"
   },
   "outputs": [],
   "source": [
    "#correlation graph for signal\n",
    "correlation_graph(sign, variables_to_draw, 'Correlations of all variables for signal (pearson)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwInaQavinph"
   },
   "source": [
    "The correlation graph of the background variables shows that cosinepos is correlated with mass. To check whether it is a real correlation or a statistical fluctuation we make our own correlation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvNhNiETxTOI"
   },
   "source": [
    "## Correlations by formula\n",
    "The following function calculates the correlation along with the standard error of the mean (SEM) of the input variable with all the other variables. The standard error of the mean is defined as $ SEM = \\frac{\\sigma}{\\sqrt{n}}$. Here $\\sigma$ is the standard deviation of a variable. It will put error bars on each bin.\n",
    "\n",
    "The function accepts 3 variables, a data frame object in the first input, a list of strings to be correlated with the third input (a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHw71Om-xVIR"
   },
   "outputs": [],
   "source": [
    "def calculate_correlation(df, vars_to_corr, target_var) :\n",
    "    \n",
    "    from scipy.stats import sem\n",
    "\n",
    "    mean = df[target_var].mean()\n",
    "    sigma = df[target_var].std()\n",
    "\n",
    "    correlation = []\n",
    "    error = []\n",
    "    \n",
    "    for j in vars_to_corr : \n",
    "        mean_j = df[j].mean()\n",
    "        sigma_j = df[j].std()\n",
    "        \n",
    "        cov = (df[j] - mean_j) * (df[target_var] - mean) / (sigma*sigma_j)        \n",
    "        correlation.append(cov.mean())\n",
    "        error.append(sem(cov))\n",
    "    \n",
    "    return correlation, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHsuo8LnxeLT"
   },
   "outputs": [],
   "source": [
    "# Provide the data frame object first, then also inside the brackets of list and then write the variable inside inverted commas ''.\n",
    "# For signal\n",
    "corr_signal, corr_signal_errors = calculate_correlation(signal, list(signal), 'mass')\n",
    "# For background\n",
    "corr_bg, corr_bg_errors = calculate_correlation(background, list(background), 'mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "VOVyyAItxkBV",
    "outputId": "210ea05a-71a6-49df-8f5a-f03b020989b8"
   },
   "outputs": [],
   "source": [
    "# Plotting the correlations of various variables with mass along with the errors\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.errorbar(list(signal), corr_signal, yerr=corr_signal_errors, fmt='')\n",
    "plt.errorbar(list(background), corr_bg, yerr=corr_bg_errors, fmt='')\n",
    "ax.grid(zorder=0)\n",
    "ax.set_xticklabels(signal.columns, fontsize=15, rotation =90)\n",
    "plt.legend(('signal','background'), fontsize = 15)\n",
    "fig.tight_layout()\n",
    "plt.title('Correlation of all variables with mass along with SEM', fontsize = 15)\n",
    "fig.savefig(\"Correlation of all variables with mass along with SEM.png\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CBM collaboration meeting correlations.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
